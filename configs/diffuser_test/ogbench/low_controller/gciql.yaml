lr: 3e-4
actor_lr: null
critic_lr: null
value_lr: null

discount: 0.9
tau: 0.005
expectile: 0.9
alpha: 1.0
actor_loss: ddpgbc  # awr æˆ– ddpgbc

actor_hidden_dims: [512, 512, 512]
value_hidden_dims: [512, 512, 512]
layer_norm: true

const_std: true
min_log_std: -5.0
max_log_std: 2.0
state_dependent_std: null

discrete: false

value_p_curgoal: 0.2  # Probability of using the current state as the value goal.
value_p_trajgoal: 0.7  # Probability of using a future state in the same trajectory as the value goal.
value_p_randomgoal: 0.1  # Probability of using a random state as the value goal.
value_geom_sample: true  # Whether to use geometric sampling for future value goals.
actor_p_curgoal: 0.0  # Probability of using the current state as the actor goal.
actor_p_trajgoal: 0.8  # Probability of using a future state in the same trajectory as the actor goal.
actor_p_randomgoal: 0.2  # Probability of using a random state as the actor goal.
actor_geom_sample: true  # Whether to use geometric sampling for future actor goals.
gc_negative: true  # Whether to use '0 if s == g else -1' (True) or '1 if s == g else 0' (False) as reward.
p_aug: 0.0  # Probability of applying image augmentation.
frame_stack: null  # Number of frames to stack.